# Service Mesh
    Definition
a service mesh is a dedicated infrastructure layer for facilitating service-to-service communications between microservices, often using a sidecar proxy.
A service mesh, like the open source project Istio, is a way to control how different parts of an application share data with one another. Unlike other systems for managing this communication, a service mesh is a dedicated infrastructure layer built right into an app. This visible infrastructure layer can document how well (or not) different parts of an app interact, so it becomes easier to optimize communication and avoid downtime as an app grows.

Service-to-service communication is what makes microservices possible. The logic governing communication can be coded into each service without a service mesh layer—but as communication gets more complex, a service mesh becomes more valuable. For cloud-native apps built in a microservices architecture, a service mesh is a way to comprise a large number of discrete services into a functional application.

    How does it work?
A service mesh doesn’t introduce new functionality to an app’s runtime environment—apps in any architecture have always needed rules to specify how requests get from point A to point B. What’s different about a service mesh is that it takes the logic governing service-to-service communication out of individual services and abstracts it to a layer of infrastructure.

To do this, a service mesh is built into an app as an array of network proxies. Proxies are a familiar concept in enterprise IT—if you are accessing this webpage from a work computer, there’s a good chance you just used one:

1. As your request for this page went out, it was first received by your company’s web proxy…
2. After passing the proxy’s security measure, it was sent to the server that hosts this page…
3. Next, this page was returned to the proxy and again checked against its security measures…
4. And then it was finally sent from the proxy to you.

In a service mesh, requests are routed between microservices through proxies in their own infrastructure layer. For this reason, individual proxies that make up a service mesh are sometimes called “sidecars,” since they run alongside each service, rather than within them. Taken together, these “sidecar” proxies—decoupled from each service—form a mesh network.
<img src="https://www.redhat.com/cms/managed-files/service-mesh-1680.png" width =500 />

Without a service mesh, each microservice needs to be coded with logic to govern service-to-service communication, which means developers are less focused on business goals. It also means communication failures are harder to diagnose because the logic that governs interservice communication is hidden within each service.

    How can a service mesh optimize communication?
Every new service added to an app, or new instance of an existing service running in a container, complicates the communication environment and introduces new points of possible failure. Within a complex microservices architecture, it can become nearly impossible to locate where problems have occurred without a service mesh.

That’s because a service mesh also captures every aspect of service-to-service communication as performance metrics. Over time, data made visible by the service mesh can be applied to the rules for interservice communication, resulting in more efficient and reliable service requests.

For example, If a given service fails, a service mesh can collect data on how long it took before a retry succeeded. As data on failure times for a given service aggregates, rules can be written to determine the optimal wait time before retrying that service, ensuring that the system does not become overburdened by unnecessary retries.

    components

A service registry is used to keep track of the location and health of services, so a requesting service can be directed to a provider service in a reliable way. Working hand in hand with the service registry is the function of service discovery. Service discovery uses the service registry’s list of available services and instances of those services, and it directs requests to the appropriate instance based on pure load balancing or any other rule that has been configured.

Some service meshes provide automated testing facilities. This gives you the ability to simulate the failure or temporary unavailability of one or more services in your app, and to make sure the resiliency features that you designed will work to keep the app functional. Two functions that can help to provide this resiliency are circuit breakers and bulkheads. Open-source mesh functions can be used to solve many of the challenges of implementing microservices.

## Service registry
The service registry is a simple key-value pair data store, maintaining a current list of all working service instances and their locations. As each service instance starts, it will register itself with the service registry, either through its own client code or through a third-party registrar function. 

<img src="https://courses.cognitiveclass.ai/assets/courseware/v1/32ba7fd1b33179b961de5d65eb2d5058/asset-v1:CognitiveClass+CO0301EN+v1+type@asset+block/dwc024_istio_service_registry.png" wideth=400 />

Because the service registry is so critical to the implementation, it needs to be made highly available in some way.

## Service discovery and service proxy
These are the functions that enable each service to find and connect with the other services it needs to do its work.
As each service instance starts, it checks in with the service registry, providing its name and location.

After this initial check-in, each service registry implementation will have some kind of heartbeat mechanism to keep its list current, removing any instances it can no longer contact. Depending on the overall design of the app, each service instance can be either a service client, requesting data or functions from other services, a service provider, providing data or functions to service clients, or both. Whenever a service needs another service, it uses the service discovery function to find an instance of that service.

    Client-side discovery
You might use round-robin load balancing where you spread each request evenly across all the instances, which sends each request to the next instance in the list. You might also use a different kind of rule, having certain clients access certain provider instances, to test new versions or for some other reason.

In any case, when you use client-side discovery, the client service is responsible for making the decision and following the decision to contact the appropriate service instance. The client will use some client code to make that happen.

In the following illustration, the requesting service instance A on the left has discovered from the service registry that there are three instances of the service it needs. Using the rule it has chosen, the requesting service has determined to contact service instance B on the right for its request.

<img src='https://courses.cognitiveclass.ai/assets/courseware/v1/72ef7b06d7beb9c48f55ed84512207c9/asset-v1:CognitiveClass+CO0301EN+v1+type@asset+block/dwc024_mesh_client_side.png' />

    Server-side discovery
This method introduces another component, labelled the load balancer in the following illustration. It is commonly called the *service proxy* in microservices discussions.

The task of querying the service registry and directing a request to the appropriate service instance is delegated to the load balancer. In some implementations, such as Istio, the load balancer can be programmed with specific rules to govern its choice of service instance, beyond just using round-robin load balancing. The nginx load balancer is built into several service meshes you might read about in the marketplace.
<img src='https://courses.cognitiveclass.ai/assets/courseware/v1/f51799d31f93211301cf4bf8afb3ee75/asset-v1:CognitiveClass+CO0301EN+v1+type@asset+block/dwc024_mesh_server_side.png' />

A major benefit of this load-balancing approach is that you need only the function of *querying the service registry* and directing requests to service instances implemented one time in the load balancer. A possible drawback is that the load balancer is one more component to manage. In most cases, this is not a problem because the load balancer is provided by the service mesh.

## Automated testing
One of the premises of cloud native and microservice architectures is that they should be designed to run on unreliable systems. You should expect components to fail and build resiliency into your apps such that they can withstand those failures. One of the basic design points of this strategy is to always have more than one instance of any given service so that your app can survive the failure of a given instance and continue to function while that instance is restarted.

Automated testing tools are useful to ensure the resiliency of your microservices app. Examples of these tools include Netflix Chaos Monkey, the Netflix Simian Army, and the features built into Istio.

With IBM Cloud Kubernetes Service, you can manage the rollout of your changes in an automated and controlled fashion. If your rollout tests aren't going well, you can roll back your deployment to the previous revision.

This flow shows how you use Kubernetes to easily manage app problems during testing:

1. Before you begin, create a deployment.
2. Roll out a change to a microservice. For example, you might want to change the image that you used in your initial deployment. When you deploy microservices with 3. Kubernetes, the change is immediately applied and logged in the roll-out history.
4. Test the status of your deployment with your operational and testing tools.
5. View the rollout history for the deployment and identify the revision number of your last deployment.
6. When ready, roll back to the previous version or specify a revision.

## Circuit breaker
The circuit breaker pattern is a programming tool included with Istio and Netflix Hystrix. Circuit breakers work to prevent response delays from a given service dependency caused by failure or latency, so that delays don’t cause broader latencies and failures throughout the app. The breaker allows you to set *timeout thresholds and failure thresholds* that allow you to fail fast from such situations with the expectation of building an alternative plan into your app.

As you make service calls through Hystrix and the thresholds are exceeded, the circuit trips and subsequent requests fail immediately. This allows you to follow the alternative plan that you should have built in to your app.

You also configure a value for these situations:

- How long a recovery time you allow before opening the circuit again
- Trying a call to the service again
- Testing your thresholds

## Bulkhead
The Bulkhead pattern is another tool included with Netflix Hystrix that prevents response delays from a given service call from causing wider problems throughout the app. This behavior is accomplished by using separate thread pools for connection attempts to other systems so that delays called by latency or failure of a given service are isolated to that service and do not use all the threads for the app as more requests are made. By using separate connection pools for each dependent service, failure on any service is isolated to that service and does not cause broader issues in the app.
